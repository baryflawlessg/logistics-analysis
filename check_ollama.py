#!/usr/bin/env python3
"""
Simple Ollama Check for Windows
==============================

This script checks if Ollama is running on Windows.
"""

import requests
import sys

def check_ollama():
    """Check if Ollama is running."""
    print("üîç Checking if Ollama is running...")
    
    try:
        # Try to connect to Ollama API
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        
        if response.status_code == 200:
            models = response.json().get('models', [])
            print("‚úÖ Ollama is running!")
            print(f"üìã Found {len(models)} models:")
            for model in models:
                print(f"   ‚Ä¢ {model.get('name', 'Unknown')}")
            return True
        else:
            print(f"‚ùå Ollama responded with error: {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("‚ùå Cannot connect to Ollama")
        print("   Ollama is not running or not accessible")
        return False
    except requests.exceptions.Timeout:
        print("‚ùå Connection timeout")
        print("   Ollama might be starting up")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def main():
    """Main function."""
    print("üß™ Ollama Status Check")
    print("=" * 30)
    
    if check_ollama():
        print("\nüéâ Ollama is ready to use!")
        print("   You can run: python llm_analyzer.py")
    else:
        print("\n‚ö†Ô∏è Ollama is not running")
        print("   To start Ollama:")
        print("   1. Open Command Prompt")
        print("   2. Run: ollama serve")
        print("   3. In another window: ollama run llama2")

if __name__ == "__main__":
    main()
