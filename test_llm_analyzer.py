#!/usr/bin/env python3
"""
Test script for LLM Analyzer
============================

This script tests the connection to Ollama and basic functionality.

Usage:
    python test_llm_analyzer.py
"""

import requests
import json

def test_ollama_connection():
    """Test if Ollama is running and accessible."""
    print("üîç Testing Ollama connection...")
    
    try:
        # Test basic connection
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"‚úÖ Ollama is running! Found {len(models)} models:")
            for model in models:
                print(f"   ‚Ä¢ {model.get('name', 'Unknown')}")
            return True
        else:
            print(f"‚ùå Ollama responded with status {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("‚ùå Cannot connect to Ollama. Is it running?")
        print("   Try: ollama serve")
        return False
    except Exception as e:
        print(f"‚ùå Error connecting to Ollama: {e}")
        return False

def test_llm_generation():
    """Test basic LLM text generation."""
    print("\nüîç Testing LLM text generation...")
    
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama2",
                "prompt": "Extract the city names from this question: Compare Chennai and Mumbai",
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            llm_response = result.get('response', '').strip()
            print(f"‚úÖ LLM responded: {llm_response[:100]}...")
            return True
        else:
            print(f"‚ùå LLM API error: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error testing LLM: {e}")
        return False

def test_json_extraction():
    """Test JSON extraction from LLM response."""
    print("\nüîç Testing JSON extraction...")
    
    prompt = """
Extract parameters from this question: "Compare Chennai and Mumbai"

Return ONLY a JSON object:
{"analysis_type": "city_comparison", "city1": "Chennai", "city2": "Mumbai", "confidence": 0.9}
"""
    
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama2",
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            llm_response = result.get('response', '').strip()
            print(f"‚úÖ LLM response: {llm_response}")
            
            # Try to extract JSON
            import re
            json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                try:
                    params = json.loads(json_str)
                    print(f"‚úÖ Extracted JSON: {params}")
                    return True
                except json.JSONDecodeError as e:
                    print(f"‚ùå JSON decode error: {e}")
                    return False
            else:
                print("‚ùå No JSON found in response")
                return False
        else:
            print(f"‚ùå LLM API error: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error testing JSON extraction: {e}")
        return False

def main():
    """Run all tests."""
    print("üß™ LLM Analyzer Test Suite")
    print("=" * 40)
    
    # Test 1: Connection
    connection_ok = test_ollama_connection()
    
    if not connection_ok:
        print("\n‚ùå Cannot proceed without Ollama connection")
        print("Please ensure Ollama is running:")
        print("  1. Install Ollama: https://ollama.ai/")
        print("  2. Start Ollama: ollama serve")
        print("  3. Load a model: ollama run llama2")
        return
    
    # Test 2: Basic generation
    generation_ok = test_llm_generation()
    
    # Test 3: JSON extraction
    json_ok = test_json_extraction()
    
    # Summary
    print("\nüìä Test Results:")
    print(f"   Connection: {'‚úÖ' if connection_ok else '‚ùå'}")
    print(f"   Generation: {'‚úÖ' if generation_ok else '‚ùå'}")
    print(f"   JSON Extract: {'‚úÖ' if json_ok else '‚ùå'}")
    
    if connection_ok and generation_ok and json_ok:
        print("\nüéâ All tests passed! LLM Analyzer is ready to use.")
        print("   Run: python llm_analyzer.py")
    else:
        print("\n‚ö†Ô∏è Some tests failed. Check the errors above.")

if __name__ == "__main__":
    main()
